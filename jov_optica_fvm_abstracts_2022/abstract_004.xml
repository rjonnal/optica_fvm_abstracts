<?xml version="1.0"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN" "http://jats.nlm.nih.gov/publishing/1.0/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="Optica Fall Vision Meeting Abstract">
	<front>
		<journal-meta>
			<journal-id journal-id-type="hwp">jov</journal-id>
			<journal-id journal-id-type="publisher-id">JOVI</journal-id>
			<journal-title-group>
				<journal-title>Journal of Vision</journal-title>
			</journal-title-group>
			<issn pub-type="epub">1534-7362</issn>
			<publisher>
				<publisher-name>Association for Research in Vision and Ophthalmology</publisher-name>
			</publisher>
		</journal-meta>
		<article-meta>
			<article-id pub-id-type="doi">10.1167/jov.23.8.4</article-id>
			<article-id pub-id-type="manuscript">Talks_4</article-id>
			<article-categories>
				<subj-group subj-group-type="category">
					<subject>Invited Session I: Vision restoration</subject>
				</subj-group>
			</article-categories>
			<title-group>
				<article-title>Invited Session I: Vision restoration: Deep learning-based stimulus optimization for prosthetic vision</article-title>
			</title-group>
			<contrib-group>
                          
				<contrib contrib-type="author">
					<name>
						<surname>Beyeler</surname>
						<given-names>Michael</given-names>
					</name>
                                        
					<xref ref-type="aff" rid="aff1"><sup>1</sup></xref>


				</contrib>




                          
				<aff id="aff1">
					<label><sup>1</sup></label>
					<institution>University of California, Santa Barbara</institution>
				</aff>


			</contrib-group>
			<pub-date pub-type="ppub"><month>9</month>
				<year>2023</year>
			</pub-date>
			<pub-date pub-type="epub"><month>9</month>
				<year>2023</year>
			</pub-date>
						<volume>23</volume>
						<issue>11</issue>
						<fpage>4</fpage>
						<lpage>4</lpage>
			<permissions>
				<license license-type="cc-by-nc-nd" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">
					<license-p>This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.</license-p>
				</license>
			</permissions>
			<abstract>
				<p>Visual neuroprostheses are emerging as a promising technology to restore a rudimentary form of vision to people living with incurable blindness. However, phosphenes elicited by current devices often appear artificial and distorted. Although current computational models can predict the neural or perceptual response to an electrical stimulus, an optimal stimulation strategy needs to solve the inverse problem: what is the required stimulus to produce a desired response? Here we frame this as an end-to-end optimization problem, where a deep neural network encoder is trained to invert a psychophysically validated phosphene model that predicts phosphene appearance as a function of stimulus amplitude, frequency, and pulse duration. As a proof of concept, we show that our strategy can produce high-fidelity, patient-specific stimuli representing handwritten digits and segmented images of everyday objects that drastically outperform conventional encoding strategies by relying on smaller stimulus amplitudes at the expense of higher frequencies and longer pulse durations. Overall, this work is an important first step towards improving visual outcomes in visual prosthesis users across a wide range of stimuli.</p>
			</abstract>
		</article-meta>
	</front>
	<back>
                Funding: NIH R00-EY029329
	</back>
</article>