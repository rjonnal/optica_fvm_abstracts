<?xml version="1.0"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN" "http://jats.nlm.nih.gov/publishing/1.0/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="Optica Fall Vision Meeting Abstract">
	<front>
		<journal-meta>
			<journal-id journal-id-type="hwp">jov</journal-id>
			<journal-id journal-id-type="publisher-id">JOVI</journal-id>
			<journal-title-group>
				<journal-title>Journal of Vision</journal-title>
			</journal-title-group>
			<issn pub-type="epub">1534-7362</issn>
			<publisher>
				<publisher-name>Association for Research in Vision and Ophthalmology</publisher-name>
			</publisher>
		</journal-meta>
		<article-meta>
			<article-id pub-id-type="doi">10.1167/jov.23.15.17</article-id>
			<article-id pub-id-type="manuscript">Talks_17</article-id>
			<article-categories>
				<subj-group subj-group-type="category">
					<subject>Invited Session IV: Extended reality--applications in vision science and beyond</subject>
				</subj-group>
			</article-categories>
			<title-group>
				<article-title>Invited Session IV: Extended reality--applications in vision science and beyond: Using a driving simulator to evaluate the effects of vision impairment and assistive technology</article-title>
			</title-group>
			<contrib-group>
                          
				<contrib contrib-type="author">
					<name>
						<surname>Bowers</surname>
						<given-names>Alex</given-names>
					</name>
                                        
					<xref ref-type="aff" rid="aff1"><sup>1</sup></xref>


				</contrib>




                          
				<aff id="aff1">
					<label><sup>1</sup></label>
					<institution>Schepens Eye Research Institute of Mass Eye and Ear, Harvard Medical School</institution>
				</aff>


			</contrib-group>
			<pub-date pub-type="ppub"><month>12</month>
				<year>2023</year>
			</pub-date>
			<pub-date pub-type="epub"><month>12</month>
				<year>2023</year>
			</pub-date>
						<volume>23</volume>
						<issue>15</issue>
						<fpage>17</fpage>
						<lpage>17</lpage>
			<permissions>
				<license license-type="cc-by-nc-nd" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">
					<license-p>This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.</license-p>
				</license>
			</permissions>
			<abstract>
				<p>Detection of potential hazards is critical to safe driving but is very difficult to evaluate in real-world driving situations because there is no control over if, when or where hazards might appear. Moreover, gaze tracking can be challenging in the varying environmental conditions of real-world driving.  In contrast, driving simulators provide a safe, controlled, repeatable environment in which to study the effects of vision impairment and assistive technologies. Scenarios can be designed to probe specific aspects of the vision loss and can include situations that would be dangerous in the real world. This talk will summarize how we have used driving simulators to evaluate gaze behaviors and driving responses to potential hazards at mid-block locations and at intersections (including gaze tracking across a 180-degree field of view) for drivers with different kinds of visual field loss and for a variety of assistive devices, including optical devices (peripheral prism glasses and bioptic telescopes) and prototype vibro-tactile hazard warning systems. Using linked pedestrian and driving simulators we have attempted to create more realistic pedestrian hazard scenarios and have evaluated the effects of vision impairment on interactions between drivers and human-controlled, interactive pedestrians within the virtual environment.</p>
			</abstract>
		</article-meta>
	</front>
	<back>
                
               <fn-group>
			<fn>
				<p>Funding: Funding: NIH grant R01 EY025677</p>
			</fn>
		</fn-group>

	</back>
</article>